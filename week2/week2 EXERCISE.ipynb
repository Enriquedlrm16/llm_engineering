{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bb45da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba2b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "ollama_client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "\n",
    "# Available models\n",
    "OPENAI_MODELS = ['gpt-4o-mini', 'gpt-4o', 'gpt-3.5-turbo']\n",
    "OLLAMA_MODELS = ['llama3.2', 'deepseek-r1:1.5b', 'codellama']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaa76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts with different expertise levels\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"Code Analyzer\": \"\"\"You are an expert code analyst. Analyze code clearly and concisely.\n",
    "Explain what each section does, identify potential issues, and suggest improvements.\n",
    "Respond in markdown format.\"\"\",\n",
    "    \n",
    "    \"Beginner Tutor\": \"\"\"You are a patient programming tutor for beginners.\n",
    "Explain code in simple terms, use analogies, and break down complex concepts.\n",
    "Encourage learning and provide helpful examples.\"\"\",\n",
    "    \n",
    "    \"Senior Developer\": \"\"\"You are a senior software engineer conducting a code review.\n",
    "Focus on architecture, design patterns, performance, security, and best practices.\n",
    "Be direct and technical in your feedback.\"\"\",\n",
    "    \n",
    "    \"Debug Assistant\": \"\"\"You are a debugging expert. Analyze code for bugs, edge cases,\n",
    "and potential runtime errors. Suggest fixes and explain why issues occur.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb0ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_connection():\n",
    "    \"\"\"Check if Ollama is running\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434\")\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568c25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_and_model(provider, model):\n",
    "    \"\"\"Get the appropriate client based on provider\"\"\"\n",
    "    if provider == \"OpenAI\":\n",
    "        return openai_client, model\n",
    "    else:  # Ollama\n",
    "        if not check_ollama_connection():\n",
    "            raise Exception(\"Ollama is not running. Please start Ollama service.\")\n",
    "        return ollama_client, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0283bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_code_stream(code_text, provider, model, expertise, temperature):\n",
    "    \"\"\"Analyze code with streaming response\"\"\"\n",
    "    try:\n",
    "        client, model_name = get_client_and_model(provider, model)\n",
    "        system_prompt = SYSTEM_PROMPTS[expertise]\n",
    "        \n",
    "        user_prompt = f\"Here is the code to analyze:\\n\\n{code_text}\\n\\nPlease provide a detailed analysis.\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                full_response += content\n",
    "                yield full_response\n",
    "\n",
    "    except Exception as e:\n",
    "        yield f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe audio to text using OpenAI Whisper\"\"\"\n",
    "    try:\n",
    "        with open(audio_path, \"rb\") as audio_file:\n",
    "            transcript = openai_client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        return f\"Error transcribing audio: {str(e)}\"\n",
    "\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"Convert text to speech using OpenAI TTS\"\"\"\n",
    "    try:\n",
    "        response = openai_client.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"alloy\",\n",
    "            input=text[:4096]  # Limit text length\n",
    "        )\n",
    "        \n",
    "        # Save to temporary file\n",
    "        output_path = \"output_speech.mp3\"\n",
    "        response.stream_to_file(output_path)\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_audio_input(audio, provider, model, expertise, temperature):\n",
    "    \"\"\"Process audio input and return both text analysis and audio response\"\"\"\n",
    "    if audio is None:\n",
    "        return \"Please record or upload audio.\", None\n",
    "    \n",
    "    # Transcribe audio\n",
    "    transcribed_text = transcribe_audio(audio)\n",
    "    \n",
    "    if transcribed_text.startswith(\"Error\"):\n",
    "        return transcribed_text, None\n",
    "    \n",
    "    # Get analysis (non-streaming for audio response)\n",
    "    try:\n",
    "        client, model_name = get_client_and_model(provider, model)\n",
    "        system_prompt = SYSTEM_PROMPTS[expertise]\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Here is the code to analyze:\\n\\n{transcribed_text}\"}\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        analysis = response.choices[0].message.content\n",
    "        \n",
    "        # Convert response to speech\n",
    "        audio_output = text_to_speech(analysis)\n",
    "        \n",
    "        return f\"**Transcribed Code:**\\n```\\n{transcribed_text}\\n```\\n\\n**Analysis:**\\n{analysis}\", audio_output\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dda1f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Gradio Interface\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Technical Code Analyzer\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üîç Technical Code Analyzer\n",
    "    ### Week 2 Exercise - Full Featured Prototype\n",
    "    Analyze code with AI assistance using text or voice input!\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Tab 1: Text Input\n",
    "        with gr.Tab(\"üìù Text Analysis\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    provider_text = gr.Radio(\n",
    "                        choices=[\"OpenAI\", \"Ollama\"],\n",
    "                        value=\"OpenAI\",\n",
    "                        label=\"Provider\"\n",
    "                    )\n",
    "                    \n",
    "                    model_text = gr.Dropdown(\n",
    "                        choices=OPENAI_MODELS,\n",
    "                        value=\"gpt-4o-mini\",\n",
    "                        label=\"Model\"\n",
    "                    )\n",
    "                    \n",
    "                    expertise_text = gr.Dropdown(\n",
    "                        choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                        value=\"Code Analyzer\",\n",
    "                        label=\"Expertise Level\"\n",
    "                    )\n",
    "                    \n",
    "                    temperature_text = gr.Slider(\n",
    "                        minimum=0,\n",
    "                        maximum=1,\n",
    "                        value=0,\n",
    "                        step=0.1,\n",
    "                        label=\"Temperature\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    code_input = gr.Code(\n",
    "                        label=\"Enter Code to Analyze\",\n",
    "                        language=\"python\",\n",
    "                        lines=15\n",
    "                    )\n",
    "                    \n",
    "                    analyze_btn = gr.Button(\"üöÄ Analyze Code\", variant=\"primary\")\n",
    "            \n",
    "            output_text = gr.Markdown(label=\"Analysis\")\n",
    "            \n",
    "            # Example codes\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"\"\"def fetch_website_links(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "    return [link for link in links if link]\"\"\"],\n",
    "                    [\"\"\"def quicksort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quicksort(left) + middle + quicksort(right)\"\"\"]\n",
    "                ],\n",
    "                inputs=code_input\n",
    "            )\n",
    "            \n",
    "            # Update model choices based on provider\n",
    "            def update_models(provider):\n",
    "                if provider == \"OpenAI\":\n",
    "                    return gr.Dropdown(choices=OPENAI_MODELS, value=\"gpt-4o-mini\")\n",
    "                else:\n",
    "                    return gr.Dropdown(choices=OLLAMA_MODELS, value=\"llama3.2\")\n",
    "            \n",
    "            provider_text.change(\n",
    "                fn=update_models,\n",
    "                inputs=provider_text,\n",
    "                outputs=model_text\n",
    "            )\n",
    "            \n",
    "            analyze_btn.click(\n",
    "                fn=analyze_code_stream,\n",
    "                inputs=[code_input, provider_text, model_text, expertise_text, temperature_text],\n",
    "                outputs=output_text\n",
    "            )\n",
    "        \n",
    "        # Tab 2: Audio Input\n",
    "        with gr.Tab(\"üé§ Voice Analysis\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Speak or upload your code question\n",
    "            The system will transcribe your voice, analyze the code, and respond with both text and audio!\n",
    "            \"\"\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    provider_audio = gr.Radio(\n",
    "                        choices=[\"OpenAI\", \"Ollama\"],\n",
    "                        value=\"OpenAI\",\n",
    "                        label=\"Provider\"\n",
    "                    )\n",
    "                    \n",
    "                    model_audio = gr.Dropdown(\n",
    "                        choices=OPENAI_MODELS,\n",
    "                        value=\"gpt-4o-mini\",\n",
    "                        label=\"Model\"\n",
    "                    )\n",
    "                    \n",
    "                    expertise_audio = gr.Dropdown(\n",
    "                        choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                        value=\"Code Analyzer\",\n",
    "                        label=\"Expertise Level\"\n",
    "                    )\n",
    "                    \n",
    "                    temperature_audio = gr.Slider(\n",
    "                        minimum=0,\n",
    "                        maximum=1,\n",
    "                        value=0,\n",
    "                        step=0.1,\n",
    "                        label=\"Temperature\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    audio_input = gr.Audio(\n",
    "                        sources=[\"microphone\", \"upload\"],\n",
    "                        type=\"filepath\",\n",
    "                        label=\"Record or Upload Audio\"\n",
    "                    )\n",
    "                    \n",
    "                    analyze_audio_btn = gr.Button(\"üéØ Analyze from Audio\", variant=\"primary\")\n",
    "            \n",
    "            output_audio_text = gr.Markdown(label=\"Analysis\")\n",
    "            output_audio_speech = gr.Audio(label=\"Audio Response\")\n",
    "            \n",
    "            provider_audio.change(\n",
    "                fn=update_models,\n",
    "                inputs=provider_audio,\n",
    "                outputs=model_audio\n",
    "            )\n",
    "            \n",
    "            analyze_audio_btn.click(\n",
    "                fn=process_audio_input,\n",
    "                inputs=[audio_input, provider_audio, model_audio, expertise_audio, temperature_audio],\n",
    "                outputs=[output_audio_text, output_audio_speech]\n",
    "            )\n",
    "        \n",
    "        # Tab 3: About\n",
    "        with gr.Tab(\"‚ÑπÔ∏è About\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ## Features\n",
    "            \n",
    "            ‚úÖ **Multiple AI Providers**: Switch between OpenAI and Ollama models\n",
    "            \n",
    "            ‚úÖ **Streaming Responses**: See analysis appear in real-time\n",
    "            \n",
    "            ‚úÖ **Custom Expertise Levels**: Choose different system prompts for various use cases\n",
    "            \n",
    "            ‚úÖ **Text & Voice Input**: Type code or speak it naturally\n",
    "            \n",
    "            ‚úÖ **Audio Output**: Get responses in both text and speech\n",
    "            \n",
    "            ‚úÖ **Temperature Control**: Adjust creativity of responses\n",
    "            \n",
    "            ## Setup Instructions\n",
    "            \n",
    "            ### Requirements:\n",
    "            ```bash\n",
    "            pip install gradio openai python-dotenv requests\n",
    "            ```\n",
    "            \n",
    "            ### Environment Variables:\n",
    "            Create a `.env` file with:\n",
    "            ```\n",
    "            OPENAI_API_KEY=your_api_key_here\n",
    "            ```\n",
    "            \n",
    "            ### For Ollama:\n",
    "            1. Install Ollama from https://ollama.ai\n",
    "            2. Run: `ollama pull llama3.2`\n",
    "            3. Run: `ollama pull deepseek-r1:1.5b`\n",
    "            4. Start Ollama service\n",
    "            \n",
    "            ## Use Cases\n",
    "            \n",
    "            - üìö **Learning Tool**: Understand complex code snippets\n",
    "            - üîç **Code Review**: Get expert feedback on your code\n",
    "            - üêõ **Debugging**: Find and fix issues\n",
    "            - üéì **Education**: Perfect for courses and tutorials\n",
    "            - üè¢ **Onboarding**: Help new developers understand codebases\n",
    "            \"\"\")\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    app.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
