{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21e58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"No se encontró OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a serious assistant that analyzes code.\n",
    "Explain clearly what each section does.\n",
    "Respond in markdown.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Here is the code. Summarize what it does:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb1a049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(code_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + code_text}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83172b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_example = \"\"\"\n",
    "def fetch_website_links(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "    return [link for link in links if link]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf58730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here’s a breakdown of the provided code:\n",
       "\n",
       "```python\n",
       "def fetch_website_links(url):\n",
       "```\n",
       "- This line defines a function named `fetch_website_links` that takes a single parameter, `url`. This parameter is expected to be a string representing the URL of a website.\n",
       "\n",
       "```python\n",
       "    response = requests.get(url, headers=headers)\n",
       "```\n",
       "- This line sends an HTTP GET request to the specified `url` using the `requests` library. The `headers` variable (which should be defined elsewhere in the code) is included in the request to provide additional information to the server (like user-agent, etc.). The server's response is stored in the variable `response`.\n",
       "\n",
       "```python\n",
       "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
       "```\n",
       "- Here, the content of the `response` (which is the HTML of the webpage) is parsed using the `BeautifulSoup` library. The `soup` variable now contains a BeautifulSoup object that allows for easy navigation and searching of the HTML structure.\n",
       "\n",
       "```python\n",
       "    links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
       "```\n",
       "- This line creates a list comprehension that iterates over all `<a>` (anchor) tags found in the `soup` object. For each anchor tag, it retrieves the value of the `href` attribute (which typically contains the URL the link points to). The resulting list of URLs is stored in the variable `links`.\n",
       "\n",
       "```python\n",
       "    return [link for link in links if link]\n",
       "```\n",
       "- Finally, this line returns a new list that includes only the non-empty links from the `links` list. It filters out any `None` or empty string values, ensuring that the returned list contains only valid URLs.\n",
       "\n",
       "### Summary\n",
       "The `fetch_website_links` function takes a URL as input, retrieves the HTML content of the webpage at that URL, extracts all the hyperlinks (URLs) from the anchor tags, and returns a list of valid (non-empty) URLs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(display(Markdown(summarize(code_example))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9ccd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b99dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(code_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + code_text}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=\"deepseek-r1:1.5b\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efe460fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided code extracts website links from a given URL using HTML parsing. Here's a step-by-step explanation:\n",
       "\n",
       "1. **Requesting the URL**: The function makes a GET request to the provided URL with headers set for authentication.\n",
       "\n",
       "2. **Parsing HTML Content**: Using BeautifulSoup, it parses the HTML content of the response page.\n",
       "\n",
       "3. **Extracting Links**: It extracts all \"a\" tags from the parsed HTML, which represent links on the website.\n",
       "\n",
       "4. **Filtering Valid Links**: The function filters out any empty or invalid URLs from the list of extracted links.\n",
       "\n",
       "5. **Returning the Result**: Finally, it returns a list containing only the valid website links.\n",
       "\n",
       "This function is typically used to retrieve actual websites that are served by an API endpoint, allowing for further processing such as data extraction or analysis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "print(display(Markdown(summarize(code_example))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27b64742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(code_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + code_text}\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28519371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Function Summary**\n",
       "=====================\n",
       "\n",
       "The `fetch_website_links` function takes a URL as input and returns a list of extracted website links.\n",
       "\n",
       "**Code Breakdown**\n",
       "-----------------\n",
       "\n",
       "### 1. `requests.get(url, headers=headers)`\n",
       "\n",
       "*   This line sends a GET request to the specified URL using the `requests` library.\n",
       "*   The `headers` parameter is used to pass additional HTTP headers with the request.\n",
       "\n",
       "### 2. `soup = BeautifulSoup(response.content, \"html.parser\")`\n",
       "\n",
       "*   After receiving the response from the server, this line parses the HTML content of the webpage using the `BeautifulSoup` library.\n",
       "*   The `\"html.parser\"` argument specifies the parser to use for parsing the HTML.\n",
       "\n",
       "### 3. `links = [link.get(\"href\") for link in soup.find_all(\"a\")]`\n",
       "\n",
       "*   This list comprehension finds all `<a>` tags (hyperlinks) within the parsed HTML and extracts their `href` attributes.\n",
       "*   The extracted links are stored in a list called `links`.\n",
       "\n",
       "### 4. `return [link for link in links if link]`\n",
       "\n",
       "*   Finally, this line filters out any empty strings from the `links` list using a conditional expression.\n",
       "*   The resulting filtered list of links is returned by the function.\n",
       "\n",
       "**Example Use Case**\n",
       "--------------------\n",
       "\n",
       "```python\n",
       "import requests\n",
       "from bs4 import BeautifulSoup\n",
       "\n",
       "# Set up headers for the request\n",
       "headers = {\"User-Agent\": \"My Web Scraper\"}\n",
       "\n",
       "# Define the URL to fetch links from\n",
       "url = \"https://www.example.com\"\n",
       "\n",
       "# Call the fetch_website_links function\n",
       "links = fetch_website_links(url)\n",
       "\n",
       "# Print the extracted links\n",
       "for link in links:\n",
       "    print(link)\n",
       "```\n",
       "\n",
       "This code snippet demonstrates how to use the `fetch_website_links` function to extract links from a given URL."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(display(Markdown(summarize(code_example))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
